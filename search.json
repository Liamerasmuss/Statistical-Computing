[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Computing Assignmnet 1",
    "section": "",
    "text": "Statistical Computing Assignment 1\nERSLIA004",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>SC Assignment1</span>"
    ]
  },
  {
    "objectID": "Part1.html",
    "href": "Part1.html",
    "title": "2  Statistical Computing - Practical1 (03Feb)",
    "section": "",
    "text": "2.1 Question 1\nFind all rows in “airquality” that have missing values. Note that the airquality dataset in R is always available (just type airquality in the console to see it).\nlibrary(tidyr)\ndata &lt;- airquality\nsum(is.na(data)) # count how many na rows\n\n[1] 44\n\ncolSums(is.na(data)) # find out which cols have na\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n\n# Find na rows in ozone\nnaOzone &lt;- c()\nfor (i in seq_along(data$Ozone)) {\n  if (is.na(data$Ozone[i])) {\n    naOzone &lt;- c(naOzone, i)\n  }\n}\nprint(naOzone)\n\n [1]   5  10  25  26  27  32  33  34  35  36  37  39  42  43  45  46  52  53  54\n[20]  55  56  57  58  59  60  61  65  72  75  83  84 102 103 107 115 119 150\n\n# Find na rows in Solar.R\nnaSolar &lt;- c()\nfor (i in seq_along(data$Solar.R)) {\n  if (is.na(data$Solar.R[i])) {\n    naSolar &lt;- c(naSolar, i)\n  }\n}\nprint(naSolar)\n\n[1]  5  6 11 27 96 97 98\n\nclean_data &lt;- data %&gt;% drop_na() # remove na rows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Computing - Practical1 (03Feb)</span>"
    ]
  },
  {
    "objectID": "Part1.html#question-1",
    "href": "Part1.html#question-1",
    "title": "2  Statistical Computing - Practical1 (03Feb)",
    "section": "",
    "text": "2.1.1 Question 2\n\n2.1.1.1 Temperature\n\n# find mean, sd, min and Max of the temperature column\nmeanTemp &lt;- mean(clean_data$Temp) \nmeanTemp\n\n[1] 77.79279\n\nsdTemp&lt;- sd(clean_data$Temp)\nsdTemp\n\n[1] 9.529969\n\nminTemp &lt;- min(clean_data$Temp)\nminTemp\n\n[1] 57\n\nmaxTemp &lt;- max(clean_data$Temp)\nmaxTemp\n\n[1] 97\n\n\n\n2.1.1.1.1 Question 2 Temp Calcs using Na.rm = True\n\n# removing all of the na rows lead to losing Temp values, redoing it to make it more accurate. Keeping the old code for learning purposes.\nmeanTemp2 &lt;- mean(data$Temp, na.rm=TRUE) \nmeanTemp2\n\n[1] 77.88235\n\nsdTemp2&lt;- sd(data$Temp, na.rm=TRUE)\nsdTemp2\n\n[1] 9.46527\n\nminTemp2 &lt;- min(data$Temp, na.rm=TRUE)\nminTemp2\n\n[1] 56\n\nmaxTemp2 &lt;- max(data$Temp, na.rm=TRUE)\nmaxTemp2\n\n[1] 97\n\n\n\n\n\n2.1.1.2 Ozone\n\n# find mean, sd, min and Max of the ozone column\nmeanOzone &lt;- mean(clean_data$Ozone)\nmeanOzone\n\n[1] 42.0991\n\nsdOzone &lt;- sd(clean_data$Ozone)\nsdOzone\n\n[1] 33.27597\n\nminOzone &lt;- min(clean_data$Ozone)\nminOzone\n\n[1] 1\n\nmaxOzone &lt;- max(clean_data$Ozone)\nmaxOzone\n\n[1] 168\n\n# all na values are in ozone, so redoing it with na.rm will make no difference\n\n\n\n\n2.1.2 Question 3\n\ndata2&lt;- cars # load cars data set\nsum(is.na(data2)) # no na rows\n\n[1] 0\n\nY &lt;- data2$dist\nX &lt;- cbind(1, data2$speed)\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y # beta estimate\nrownames(beta_hat) &lt;- c(\"Intercept\", \"Speed\")\nbeta_hat\n\n                [,1]\nIntercept -17.579095\nSpeed       3.932409\n\n\n\n\n2.1.3 Question 4\n\nlm(Y~X) #check against lm function\n\n\nCall:\nlm(formula = Y ~ X)\n\nCoefficients:\n(Intercept)           X1           X2  \n    -17.579           NA        3.932  \n\n# get the same value = question 3 calc was correc",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Computing - Practical1 (03Feb)</span>"
    ]
  },
  {
    "objectID": "Part2.html",
    "href": "Part2.html",
    "title": "3  Statistical Computing- Exercie05Feb",
    "section": "",
    "text": "4 Question 4 - Create Sin Graph\n\ncurve(sin(x), from = -2, to = 2, col = \"blue\", xlab = \"x\", ylab = \"sin(x)\", main = \"Sin Graph\")\n\n\n\n\n\n\n\n# create sin graph ranging from -2 to 2      \n\n\n\n5 Question 5\n\nvals &lt;- rt(1000, 1) # 1000 t distribution vals with 1df\nnorms&lt;- sort(rnorm(500000, 0, 1)) # 500 000 norm dist values\n\nindices &lt;- ceiling(seq(from = 1,\n               to = 500000,\n               length.out = 1000))\n# Create 1000 equally spaced points from 1 to 500000\n# and apply ceiling() to ensure all values are valid integer indices\n\n\nreduced_norms &lt;- norms[indices]\n# Subset the sorted normal sample using the generated indices\n# to obtain 1000 evenly spaced reference values\nsorted_vals &lt;- sort(vals)\n# Sort the simulated values in ascending order\n# to align them with the theoretical reference values\n\nsim_matrix &lt;-  replicate(1000, sort(rnorm(1000, 0, 1))) # 1000 ind. samples of size 1000 from std normal dist. Sorted and then stored\n\nlower_env &lt;- apply(sim_matrix, 1, quantile, probs = 0.025) # calc the 2.5th percentile across the simulated samples to make the lower \"envelope\"\nupper_env &lt;- apply(sim_matrix, 1, quantile, probs = 0.975) # calc the 97.5th percentile across the simulated samples to make the lower \"envelope\"\n\ntheoretical_q &lt;- rowMeans(sim_matrix) # estimate the theoretical quantiles \n\n# plot it all\nplot(reduced_norms,\n     sorted_vals, ylim = c(-4, 4), ylab = \"vals\", xlab=\"norm quantiles\")\nabline(0,1)\nlines(theoretical_q, lower_env, col = \"red\", lwd = 2)\nlines(theoretical_q, upper_env, col = \"green\", lwd = 1)\n\n\n\n\n\n\n\n# check\nlibrary(car)\n\nLoading required package: carData\n\nqqPlot(vals, envelope = 0.95, ylim = c(-5, 5))\n\n\n\n\n\n\n\n\n[1]  62 657\n\n# plot looks correct",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical Computing- Exercie05Feb</span>"
    ]
  },
  {
    "objectID": "Part3.html",
    "href": "Part3.html",
    "title": "4  Statistical Computing- Day4",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.2     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n4.0.1 Question 1\nDisplay the flights dataset in an alternative format to simply printing it (i.e. running flights).\n\nlibrary(nycflights13)\n\ndplyr::glimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\n\n\n4.0.2 Question 2\nRewrite this code using dplyr and the pipe:\n\n# rewrite this code in tidyverse and the pipe\nflight1 &lt;- flights[flights$month == 1, ]\ncarrier_vec &lt;- unique(flight1$carrier)\ncarrier_dist_vec_mean &lt;- numeric(length(carrier_vec))\ncarrier_dist_vec_sd &lt;- numeric(length(carrier_vec))\nfor (i in seq_along(carrier_vec)) {\n  carrier_dist_vec_mean[i] &lt;- mean(\n    flight1$distance[flight1$carrier == carrier_vec[i]]\n   )\n  carrier_dist_vec_sd[i] &lt;- sd(\n    flight1$distance[flight1$carrier == carrier_vec[i]]\n  )\n}\ndist_tbl &lt;- tibble(\n  carrier = carrier_vec,\n  mean_distance = carrier_dist_vec_mean,\n  sd_distance = carrier_dist_vec_sd\n)\ndist_tbl[order(dist_tbl$mean_distance), ]\n\n# A tibble: 16 × 3\n   carrier mean_distance sd_distance\n   &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n 1 YV               229          0  \n 2 9E               476.       334. \n 3 EV               522.       294. \n 4 US               536.       553. \n 5 MQ               566.       223. \n 6 FL               691.       142. \n 7 OO               733         NA  \n 8 WN               942.       496. \n 9 B6              1062.       681. \n10 DL              1220.       644. \n11 AA              1350.       626. \n12 UA              1462.       778. \n13 F9              1620          0  \n14 AS              2402          0  \n15 VX              2495.        98.2\n16 HA              4983          0  \n\n\n\ndist_tbl &lt;- flights %&gt;% \n  filter(month==1) %&gt;%\n  group_by(carrier) %&gt;%\n  summarise(\n    mean_distance = mean(distance, na.rm=TRUE),\n    sd_distance = sd(distance, na.rm= TRUE)\n  ) %&gt;%\n  arrange(mean_distance)\ndist_tbl\n\n# A tibble: 16 × 3\n   carrier mean_distance sd_distance\n   &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n 1 YV               229          0  \n 2 9E               476.       334. \n 3 EV               522.       294. \n 4 US               536.       553. \n 5 MQ               566.       223. \n 6 FL               691.       142. \n 7 OO               733         NA  \n 8 WN               942.       496. \n 9 B6              1062.       681. \n10 DL              1220.       644. \n11 AA              1350.       626. \n12 UA              1462.       778. \n13 F9              1620          0  \n14 AS              2402          0  \n15 VX              2495.        98.2\n16 HA              4983          0  \n\n\n\n\n4.0.3 Question 3\nExplain why the standard deviation is NA for one carrier, and why it is 0 for others. Demonstrate your answer using code.\n\nflight1 %&gt;%\n  count(carrier)\n\n# A tibble: 16 × 2\n   carrier     n\n   &lt;chr&gt;   &lt;int&gt;\n 1 9E       1573\n 2 AA       2794\n 3 AS         62\n 4 B6       4427\n 5 DL       3690\n 6 EV       4171\n 7 F9         59\n 8 FL        328\n 9 HA         31\n10 MQ       2271\n11 OO          1\n12 UA       4637\n13 US       1602\n14 VX        316\n15 WN        996\n16 YV         46\n\nflight1 %&gt;%\n    filter(carrier==\"F9\") %&gt;%\n    select(distance)\n\n# A tibble: 59 × 1\n   distance\n      &lt;dbl&gt;\n 1     1620\n 2     1620\n 3     1620\n 4     1620\n 5     1620\n 6     1620\n 7     1620\n 8     1620\n 9     1620\n10     1620\n# ℹ 49 more rows\n\n\nANSWER: The sd is NA for one carrier because there is only one value listed for that carrier (OO). The sd is zero for others because they all traveled the same distance each time, which leads to an sd of zero. (ie. F9 consistently traveled 1620)\n\n\n4.0.4 Question 4\nUsing tidyr and dplyr where appropriate, construct a dataframe where the carriers are along the columns, and the rows are the average departure delay (dep_delay) flown by each carrier (carrier) in each month.\n\ndelay_tbl &lt;- flights %&gt;%\n  group_by(month, carrier) %&gt;%\n  summarise(\n    mean_dep_delay = mean(dep_delay, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  pivot_wider(\n    names_from  = carrier,\n    values_from = mean_dep_delay\n  )\n\ndelay_tbl\n\n# A tibble: 12 × 17\n   month  `9E`    AA     AS    B6    DL    EV    F9    FL    HA    MQ    OO\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 16.9   6.93  7.35   9.49  3.85 24.2  10     1.97 54.4   6.49 67   \n 2     2 16.5   8.28  0.722 13.8   5.54 21.5  29.8   5.18 17.4   8.09 NA   \n 3     3 13.4   8.70  8.42  14.2   9.93 26.2  16.8  17.3   1.16  7.19 NA   \n 4     4 13.6  11.7  11.3   15.2   8.17 22.8  24.6  13.1  -2.1  13.7  NA   \n 5     5 22.7   9.66  6.77   9.78  9.74 20.2  35.9  19.2  -1.45 13.9  NA   \n 6     6 29.0  14.6  13.1   20.4  18.7  25.5  29.4  38.8   1.47 20.8  61   \n 7     7 31.4  12.1   2.42  24.9  20.6  26.5  31.8  41.2  -1.71 20.7  NA   \n 8     8 17.3   7.17  2.87  15.7   9.85 16.3  22.2  23.4   1.68 10.1  64   \n 9     9  7.75  5.69 -4.52   6.63  5.53  8.24  8.26 16.9  -5.44  5.35 -4.94\n10    10  9.33  3.00  0.677  2.96  3.42 13.4   9.70 13.7  -5.10  4.48 NA   \n11    11  7.56  3.10  3.08   3.52  2.85  9.83 13.5  16.9  -5.44  3.28  0.8 \n12    12 19.8  11.7  18.0   17.0  10.8  27.9  13.1  26.1  -3.14 12.7  NA   \n# ℹ 5 more variables: UA &lt;dbl&gt;, US &lt;dbl&gt;, VX &lt;dbl&gt;, WN &lt;dbl&gt;, YV &lt;dbl&gt;\n\n\n\n\n4.0.5 Question 5\nCalculate the proportion of flights that were delayed (dep_delay greater than 0) but arrived on or before time (arr_delay less than or equal to 0).\n\nprop_TBL &lt;- flights %&gt;% \n  summarise(\n    proportion = mean(dep_delay&gt;0 & arr_delay&lt;= 0, na.rm = TRUE)\n  )\n\nprop_TBL\n\n# A tibble: 1 × 1\n  proportion\n       &lt;dbl&gt;\n1      0.108\n\n\n\n\n4.0.6 Question 6\nUsing the airlines and flights datasets, do the following, showing the output from each step:\n\nIdentify routes that more than one airline flies\n\n\nrouteMultAirlines &lt;- flights %&gt;% \n  group_by(origin, dest) %&gt;% \n  summarise(\n    carriers = n_distinct(carrier),\n    .groups = \"drop\"\n  ) %&gt;% \n  filter(carriers&gt;1)\n  routeMultAirlines\n\n# A tibble: 128 × 3\n   origin dest  carriers\n   &lt;chr&gt;  &lt;chr&gt;    &lt;int&gt;\n 1 EWR    ATL          4\n 2 EWR    AUS          2\n 3 EWR    BDL          2\n 4 EWR    BNA          2\n 5 EWR    BOS          3\n 6 EWR    BWI          2\n 7 EWR    CHS          2\n 8 EWR    CLE          2\n 9 EWR    CLT          3\n10 EWR    CVG          2\n# ℹ 118 more rows\n\n\n\nFor each such route, calculate the average arrival delay for each airline (exclude NAs). Find the names of these airlines.\n\n\nmultiRoutes &lt;- flights %&gt;% \n  group_by(origin, dest) %&gt;% \n  summarise(\n    n_carriers = n_distinct(carrier), .groups = \"drop\") %&gt;%\n  filter(n_carriers &gt;1)\n\nAvgArrDelay &lt;- flights %&gt;% \n  semi_join(multiRoutes, by = c(\"origin\", \"dest\")) %&gt;% \n  group_by(origin, dest, carrier) %&gt;% \n  summarise(\n    AvgArrDelay = mean(arr_delay, na.rm = TRUE),\n    .groups = \"drop\"\n  )\nAvgArrDelay\n\n# A tibble: 343 × 4\n   origin dest  carrier AvgArrDelay\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 EWR    ATL   9E            -6.25\n 2 EWR    ATL   DL            10.00\n 3 EWR    ATL   EV            19.5 \n 4 EWR    ATL   UA            10.5 \n 5 EWR    AUS   UA             4.28\n 6 EWR    AUS   WN           -11.2 \n 7 EWR    BDL   EV             6.78\n 8 EWR    BDL   UA            22.6 \n 9 EWR    BNA   EV            17.7 \n10 EWR    BNA   WN            -2.13\n# ℹ 333 more rows\n\n\n\nFor each such route, identify the airline with the worst and best average arrival delay.\n\nbest_worst &lt;- AvgArrDelay %&gt;%\n  group_by(origin, dest) %&gt;%\n  summarise(\n    best_airline  = carrier[which.min(AvgArrDelay)],\n    best_delay    = min(AvgArrDelay),\n    worst_airline = carrier[which.max(AvgArrDelay)],\n    worst_delay   = max(AvgArrDelay),\n    .groups = \"drop\"\n  )\n\nbest_worst\n\n# A tibble: 128 × 6\n   origin dest  best_airline best_delay worst_airline worst_delay\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;\n 1 EWR    ATL   9E               -6.25  EV                  19.5 \n 2 EWR    AUS   WN              -11.2   UA                   4.28\n 3 EWR    BDL   EV                6.78  UA                  22.6 \n 4 EWR    BNA   WN               -2.13  EV                  17.7 \n 5 EWR    BOS   EV               -4.01  B6                   6.87\n 6 EWR    BWI   WN                5.95  EV                  20.1 \n 7 EWR    CHS   UA              -14     EV                  16.2 \n 8 EWR    CLE   EV               -3.71  UA                   5.97\n 9 EWR    CLT   US                0.920 EV                  20.5 \n10 EWR    CVG   9E                1.40  EV                  21.2 \n# ℹ 118 more rows\n\n\nIdentify the route with the greatest difference between the best and worst performing airlines\n\n\nGreatestDiff &lt;- AvgArrDelay %&gt;%\n  group_by(origin, dest) %&gt;%\n  summarise(\n    best_delay    = min(AvgArrDelay),\n    worst_delay   = max(AvgArrDelay),\n    diff = worst_delay - best_delay,\n    .groups = \"drop\"\n  )\n\nGreatestDiff %&gt;%\n  slice_max(diff, n = 1)\n\n# A tibble: 1 × 5\n  origin dest  best_delay worst_delay  diff\n  &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 JFK    ATL         1.40         128  127.\n\n\n\nDetermine the reason for this difference\n\nThe difference in average arrival delay on this route is mainly explained by differences in departure delays. The airline with the worst arrival performance also departs later on average, which suggests that the delay originates before takeoff rather than during the flight itself. There is no strong evidence that in-air time is the primary driver of the gap.Question 7\nIdentify all columns with missing entries, typos and any other inconsistencies in the dataset.\n\nque7 &lt;- structure(list(id = c(\"id_1\", \"id_2\", \"id_3\", \"id_4\", \"id_5\", \n\"id_6\", \"id_7\", \"id_8\", \"id_9\", \"id_10\", \"id_11\", \"id_12\", \"id_13\", \n\"id_14\", \"id_15\", \"id_16\", \"id_17\", \"id_18\", \"id_19\", \"id_20\", \n\"id_21\", \"id_22\", \"id_23\", \"id_24\", \"id_25\", \"id_26\", \"id_27\", \n\"id_28\", \"id_29\", \"id_30\", \"id_31\", \"id_32\", \"id_33\", \"id_34\", \n\"id_35\", \"id_36\", \"id_37\", \"id_38\", \"id_39\", \"id_40\", \"id_41\", \n\"id_42\", \"id_43\", \"id_44\", \"id_45\", \"id_46\", \"id_47\", \"id_48\", \n\"id_49\", \"id_50\"), age = c(50L, 34L, 70L, 33L, 22L, 61L, 69L, \n73L, 62L, 56L, 71L, 33L, 73L, 44L, 45L, 46L, 24L, 70L, 46L, 76L, \n47L, 76L, 28L, 48L, 54L, 27L, 45L, 26L, 61L, 28L, 38L, 55L, 33L, \n36L, 62L, 58L, 72L, 31L, 34L, 51L, 61L, 64L, 26L, 28L, 60L, 29L, \n42L, 46L, 79L, 72L), gender = c(\"male\", \"male\", \"male\", \"female\", \n\"female\", \"male\", \"female\", \"male\", \"male\", \"female\", \"female\", \n\"male\", \"male\", \"female\", \"male\", \"male\", \"male\", \"male\", \"female\", \n\"male\", \"male\", \"male\", \"male\", \"female\", \"femal\", \"male\", \"female\", \n\"female\", \"female\", \"female\", \"male\", \"female\", \"female\", \"female\", \n\"male\", \"male\", \"female\", \"male\", \"female\", \"female\", \"male\", \n\"female\", \"female\", \"male\", \"male\", \"female\", \"male\", \"male\", \n\"male\", \"female\"), height = c(174.4, 197.7, 174.1, 194.5, NA, \n180.4, 170.5, 157.4, 196.8, 165.1, 153, 197.4, 186, 157.1, 177.5, \n197.7, 179.3, 170.2, 182.4, NA, 165.4, 161, 168.5, 199.2, 157.7, \n154.6, 157.1, 184.5, 181, 194.6, 183.6, 186.9, 176.1, 183, 191.1, \n189.3, 199, 172, 165.6, 170.5, 150.5, 159.2, 192.1, 161.6, 162, \n153.8, 162.3, 186.6, 192.4, 174.9), weight = c(69.4, 62.3, 55.6, \n69.5, 78.6, 60.8, 72.2, 60.9, 75.1, 67.7, 82.5, 68.7, 67.8, 76.7, \n87, 61.1, 70.6, 63.3, 81.5, 59.2, 93.2, 87.3, 83.4, 80.9, 68.6, \n76.5, 93.7, 79.1, 92, 65.6, 85.4, 63.3, 79.7, 74.1, 63.3, 78.2, \n95.7, 95.1, 63.7, 66.1, 99.3, 81, 96.9, 73.3, 70.3, 83, 57.6, \n78.6, 61.9, 98.1), blood_type = c(\"O\", \"A\", \"O\", \"O\", \"B\", \"AB\", \n\"O\", \"O\", \"O\", \"AB\", \"A\", \"O\", \"O\", \"O\", \"B\", \"A\", \"B\", \"AB\", \n\"O\", \"AB\", \"A\", \"AB\", \"O\", \"B\", \"A\", \"A\", \"B\", \"AB\", \"A\", \"B\", \n\"B\", \"A\", \"O\", \"O\", \"O\", \"B\", \"O\", \"A\", \"A\", \"B\", \"A\", \"O\", \"AB\", \n\"A\", \"A\", \"O\", \"O\", \"B\", \"A\", \"O\"), disease_status = c(\"diseased\", \n\"healthy\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \"diseased\", \n\"healthy\", \"diseased\", \"Healthy\", \"diseased\", \"healthy\", \"diseased\", \n\"healthy\", \"diseased\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \n\"healthy\", \"healthy\", \"diseased\", \"healthy\", \"diseased\", \"healthy\", \n\"healthy\", \"healthy\", \"healthy\", \"diseased\", \"diseased\", \"healthy\", \n\"healthy\", \"healthy\", \"diseased\", \"diseased\", \"diseased\", \"healthy\", \n\"diseased\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \n\"diseased\", \"diseased\", \"diseased\", \"healthy\", \"healthy\", \"diseased\", \n\"diseased\"), cholesterol = c(228, 223, 213, 198, 166, 151, 195, \n199, 189, 196, 221, 156, 185, 230, 234, 174, 185, 236, 235, 180, \n165, 220, 160, 153, 250, 153, 184, 242, 212, 179, 224, 233, 181, \n199, 220, 214, 214, 248, 191, 162, 203, 173, 199, 187, 248, 189, \n173, 212, 164, 247), glucose = c(96, 78, 101, 119, 103, 91, 86, \nNA, 77, 80, 115, 85, 88, 109, NA, 71, 90, 94, 91, 87, 113, 93, \n97, 118, 109, 80, 85, 119, 99, 108, 89, 108, 97, 116, 79, 84, \n75, 81, 119, NA, 106, 109, 75, 82, 84, 75, 76, 120, 119, 77), \n    smoker = c(\"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"no\", \n    \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \n    \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \n    \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \n    \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \n    \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\"), exercise = c(\"occasional\", \n    \"regular\", \"occasional\", \"regular\", \"none\", \"occasional\", \n    \"regular\", \"none\", \"occasional\", \"none\", \"occasional\", \"none\", \n    \"none\", \"regular\", \"occasional\", \"none\", \"regular\", \"regular\", \n    \"none\", \"occasional\", \"none\", \"occasional\", \"occasional\", \n    \"occasional\", \"regular\", \"occasional\", \"regular\", \"regular\", \n    \"regular\", \"occasional\", \"occasional\", \"none\", \"none\", \"regular\", \n    \"occasional\", \"occasional\", \"none\", \"none\", \"none\", \"none\", \n    \"occasional\", \"regular\", \"regular\", \"none\", \"regular\", \"occasional\", \n    \"occasional\", \"none\", \"occasional\", \"regular\"), income = c(84820L, \n    81547L, 22588L, 72490L, 74533L, 25338L, 41469L, 57315L, 63629L, \n    88662L, 62615L, 56261L, 58499L, 82232L, 77584L, 77275L, 38468L, \n    54510L, 91326L, 78611L, 31402L, 29586L, 21441L, 58269L, 84173L, \n    88295L, 37940L, 43750L, 69750L, 92356L, 82518L, 91455L, 68866L, \n    51178L, 68275L, 27689L, 35418L, 81318L, 62405L, 86851L, 25654L, \n    47553L, 74474L, 51409L, 22607L, 55360L, 96351L, 21516L, 41927L, \n    55810L), education = c(\"master\", \"bachelor\", \"PhD\", \"master\", \n    \"bachelor\", \"highschool\", \"PhD\", \"highschool\", \"PhD\", \"PhD\", \n    \"bachelor\", \"highschool\", \"master\", \"bachelor\", \"PhD\", \"PhD\", \n    \"PhD\", \"bachelor\", \"master\", \"highschool\", \"PhD\", \"highschool\", \n    \"bachelor\", \"master\", \"highschool\", \"highschool\", \"master\", \n    \"master\", \"bachelor\", \"PhD\", \"highschool\", \"PhD\", \"master\", \n    \"master\", \"master\", \"PhD\", \"highschool\", \"master\", \"master\", \n    \"highschool\", \"bachelor\", \"highschool\", \"bachelor\", \"PhD\", \n    \"bachelor\", \"highschool\", \"master\", \"highschool\", \"bachelor\", \n    \"bachelor\"), region = c(\"North\", \"South\", \"North\", \"West\", \n    \"North\", \"West\", \"South\", \"South\", \"West\", \"South\", \"West\", \n    \"South\", \"West\", \"East\", \"North\", \"West\", \"North\", \"North\", \n    \"West\", \"North\", \"East\", \"West\", \"South\", \"North\", \"North\", \n    \"East\", \"East\", \"North\", \"North\", \"West\", \"South\", \"West\", \n    \"West\", \"East\", \"West\", \"North\", \"West\", \"North\", \"East\", \n    \"North\", \"West\", \"South\", \"South\", \"East\", \"North\", \"West\", \n    \"West\", \"East\", \"North\", \"East\"), marital_status = c(\"divorced\", \n    \"single\", \"divorced\", \"divorced\", \"divorced\", \"divorced\", \n    \"divorced\", \"married\", \"divorced\", \"married\", \"divorced\", \n    \"widowed\", \"married\", \"single\", \"widowed\", \"widowed\", \"single\", \n    \"divorced\", \"widowed\", \"widowed\", \"single\", \"married\", \"single\", \n    \"married\", \"widowed\", \"married\", \"single\", \"single\", \"widowed\", \n    \"married\", \"widowed\", \"divorced\", \"single\", \"married\", \"single\", \n    \"widowed\", \"widowed\", \"married\", \"widowed\", \"divorced\", \"married\", \n    \"married\", \"divorced\", \"single\", \"married\", \"widowed\", \"divorced\", \n    \"divorced\", \"single\", \"divorced\")), row.names = c(NA, -50L\n), class = c(\"tbl_df\", \"tbl\", \"data.frame\"))\n\nque7 %&gt;% \n  count(gender) # misspelled \"femal\"\n\n# A tibble: 3 × 2\n  gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 femal      1\n2 female    22\n3 male      27\n\nque7 %&gt;% \n  count(disease_status) # inconsistent capitalisation\n\n# A tibble: 3 × 2\n  disease_status     n\n  &lt;chr&gt;          &lt;int&gt;\n1 Healthy            1\n2 diseased          19\n3 healthy           30\n\nque7 %&gt;%\n  summarise(across(everything(), n_distinct))\n\n# A tibble: 1 × 15\n     id   age gender height weight blood_type disease_status cholesterol glucose\n  &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;      &lt;int&gt;          &lt;int&gt;       &lt;int&gt;   &lt;int&gt;\n1    50    34      3     46     47          4              3          40      34\n# ℹ 6 more variables: smoker &lt;int&gt;, exercise &lt;int&gt;, income &lt;int&gt;,\n#   education &lt;int&gt;, region &lt;int&gt;, marital_status &lt;int&gt;\n\n# 4 marital status seems correct\n# 4 regions seems correct\n# 4 education seems correct\n# all incomes are unique which is seemingly correct\n# 3 exercise seems correct\n# 2 for smoker is correct\n# glucose seems fine\n# 4 blood types is correct\n\nque7 %&gt;% \n  summarise(across(everything(), ~sum(is.na(.)))) # 2 na values in height and 3 in glucose\n\n# A tibble: 1 × 15\n     id   age gender height weight blood_type disease_status cholesterol glucose\n  &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;      &lt;int&gt;          &lt;int&gt;       &lt;int&gt;   &lt;int&gt;\n1     0     0      0      2      0          0              0           0       3\n# ℹ 6 more variables: smoker &lt;int&gt;, exercise &lt;int&gt;, income &lt;int&gt;,\n#   education &lt;int&gt;, region &lt;int&gt;, marital_status &lt;int&gt;\n\n# overall errors in gender, disease_status, height and glucose.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical Computing- Day4</span>"
    ]
  }
]